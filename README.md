# llama-stack
Hybrid CPU+GPU LLM inference with LLaMA.cpp

docker compose -f compose.bench.yml up llama_cpp_bench
