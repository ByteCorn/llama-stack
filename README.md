# llama-stack
Hybrid CPU+GPU LLM inference with LLaMA.cpp

## проверки
docker compose -f compose.bench.yml up llama_cpp_bench 2>&1 | tee benchmark_results.log

