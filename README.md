# llama-stack
Hybrid CPU+GPU LLM inference with LLaMA.cpp
