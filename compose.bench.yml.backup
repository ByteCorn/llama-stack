x-llm-base: &llm-base
  image: ghcr.io/ggml-org/llama.cpp:full-cuda
  user: "${UID:-1000}:${GID:-1000}"
  volumes:
    - ./models:/models:ro
    - ./cache:/cache
    - ./corpus:/corpus:ro
    - ./results:/results
    - ./benchmark.sh:/benchmark.sh
  cap_add:
    - IPC_LOCK
    - SYS_NICE  # для приоритизации процессов
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nproc:
      soft: 65535
      hard: 65535
  shm_size: "2gb"  # достаточно для IPC
  security_opt:
    - label=disable
    - seccomp=unconfined
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu, utility]
      limits:
        memory: 56G  # дставляю 8GB для системы
        cpus: '10.0' # фиксирую CPU
  restart: "no"
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8080}/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s

services:
  llama_cpp_benchmark:
    <<: *llm-base
    container_name: llama_cpp_benchmark
    entrypoint: [ "/bin/bash", "/benchmark.sh" ]
    environment:
      # Конфигурация сервера
      - LLAMA_ARG_HOST=0.0.0.0 # сервер слушает на всех интерфейсах
      - LLAMA_ARG_PORT=8080 # порт для подключения
      - LLAMA_ARG_THREADS_HTTP=16 # количество потоков для обработки HTTP-запросов
      # Конфигурация модели
      - LLAMA_ARG_MODEL=/models/Qwen2.5-Coder-32B-Instruct-abliterated-Q5_K_M.gguf # путь к модели
      - LLAMA_ARG_ALIAS=qwencoder # псевдоним модели
      # Параллелизм и потоки
      - LLAMA_ARG_THREADS=12 # потоков для вычислений
      - LLAMA_ARG_THREADS_BATCH=12 # потоков для пакетной обработки
      - LLAMA_ARG_N_PARALLEL=4 # параллельных запросов
      # Контекст и память
      - LLAMA_ARG_CTX_SIZE=32768 # размер контекста
      - LLAMA_ARG_NO_MMAP=false # используем маппинг файлов в память
      # GPU настройки
      - LLAMA_ARG_N_GPU_LAYERS=99 # загружаем 38 слоёв модели на GPU
      # Дополнительные настройки
      - LLAMA_ARG_NO_KV_OFFLOAD=false # включаем offload KV для лучшего использования GPU
      - LLAMA_ARG_MLOCK=true # форсимируем систему держать модель в RAM, чтобы избежать свопинга и сжатия
      # Дополнительные оптимизации
      - LLAMA_ARG_FLASH_ATTN=auto # используем Flash Attention, если поддерживается
      - LLAMA_ARG_KV_UNIFIED=true # единый буфер кэша KV для всех последовательностей
    entrypoint: ["/bin/bash", "/run_tests.sh"]
    volumes:
      - ./models:/models:ro
      - ./corpus:/corpus:ro
      - ./results:/results
      - ./cache:/cache
    ports:
      - "8080:8080"
    cap_add:
      - IPC_LOCK
    ulimits:
      memlock:
        soft: -1
        hard: -1
    shm_size: "64gb"
    security_opt:
      - label=disable
      - seccomp=unconfined
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    restart: unless-stopped
