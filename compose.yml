services:
  qwen-server:
    image: ghcr.io/ggerganov/llama.cpp:full-cuda-12
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./models:/models
      - ./cache:/build/cache
    ports:
      - "8080:8080"
    command: >
      --server
      -m /models/qwen2.5-32b-instruct-q5_k_m.gguf
      --host 0.0.0.0 --port 8080
      --ctx-size 16384 --batch-size 512
      --n-gpu-layers 38
      --mlock 40
      --tensor-split "0,100"
      --threads 12
      --rpc
      --embedding
      --log-disable
      --verbose-prompt
  