services:
  qwen-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    environment:
      # Model Configuration
      - LLAMA_ARG_MODEL=/models/qwen2.5-coder-32b-instruct-q5_k_m.gguf
      - LLAMA_ARG_ALIAS=qwencoder
      # Server Configuration
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8080
      - LLAMA_ARG_CTX_SIZE=2048
      - LLAMA_ARG_N_PARALLEL=4
      # Embeddings-specific
      - LLAMA_ARG_EMBEDDINGS=true
      - LLAMA_ARG_POOLING=mean
      # Performance
      - LLAMA_ARG_THREADS=-1
      - LLAMA_ARG_NO_MMAP=false
      # GPU support
      - LLAMA_ARG_N_GPU_LAYERS=40
    volumes:
      - ./models:/models
      - ./cache:/build/cache
    ports:
      - "8080:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
